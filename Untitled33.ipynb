{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZRFC0scBzNBZ1pl2u8bT0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaronke/COMPUTATION/blob/main/Untitled33.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import nltk\n",
        "nltk.download('wordnet')\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXLcQuFcwnIJ",
        "outputId": "229d9129-7989-47ef-abd2-7c9d45ff251f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import nltk\n",
        "nltk.download('wordnet')\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqh0sNowwtn5",
        "outputId": "605f0b0a-5308-4db6-dfb4-a377fce05375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " >>> import nltk\n",
        "nltk.download('omw-1.4')\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIbv5n03w17m",
        "outputId": "7818a919-0e1b-4fcb-da89-c9c5469d217b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import nltk\n",
        "nltk.download('omw-1.4')\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbgtW8tXw-5c",
        "outputId": "bd544e7d-1873-43e4-e345-dee0fd8f7375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Os9a0l4xIa9",
        "outputId": "dd80bd4c-7f9d-4f23-a162-ff9ef3d864e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAtGloNbvWYj",
        "outputId": "fd8c0b65-abf5-45f3-c791-f09dd996ddbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Alpha   =  0.3 \n",
            "\n",
            "1                        cost = 0.6931471805599454\n",
            "50                       cost = 0.22375373819951783\n",
            "100                      cost = 0.15415648064912277\n",
            "150                      cost = 0.12281622350227726\n",
            "200                      cost = 0.10401551989582403\n",
            "250                      cost = 0.09112190013799568\n",
            "\n",
            "\n",
            "                              Positive prediction Negative prediction  Tests\n",
            "   ham  = 0   Actual negative          FP=2                TN=95          97\n",
            "   spam = 1   Actual positive          TP=24               FN=5           29\n",
            "\n",
            "\n",
            "                precision recall f1-score support\n",
            "          ham 0      0.95   0.98     0.96      97\n",
            "         spam 1      0.92   0.83     0.87      29\n",
            "                                                 \n",
            "       accuracy                      0.94     126\n",
            "      macro avg      0.94    0.9     0.92     126\n",
            "   weighted avg      0.94   0.94     0.94     126\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from get_text import *\n",
        "def get_text_from_txt(file, encode=\"latin-1\"):    \n",
        "    txt = open(file, \"r\", encoding=encode)\n",
        "    text = txt.read()\n",
        "    txt.close()\n",
        "    \n",
        "    return text\n",
        "#from normalize_text import *\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "        \n",
        "def normalize_text(text):\n",
        "    validCharacters=['á','é','í','ó','ú','ü','a','b','c','d','e','f','g','h','i','j','k','l','m','n','ñ','o','p','q','r','s','t','u','v','w','x','y','z', ' ', '\\n']\n",
        "    \n",
        "    # Pasar a minúsculas\n",
        "    normalizeText=text.lower()\n",
        "    \n",
        "    # Quitar números y caracteres no alfanuméricos\n",
        "    normalizeText=''.join(c for c in normalizeText if c in validCharacters)\n",
        "    \n",
        "    # Lematizar\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    result=''\n",
        "    lines=normalizeText.split('\\n')\n",
        "    numberLines=len(lines)\n",
        "    for i in range(numberLines):\n",
        "        for w in lines[i].split():\n",
        "            result+=lemmatizer.lemmatize(w)+' '\n",
        "        if (i < (numberLines-1)):\n",
        "            result+='\\n'\n",
        "    \n",
        "    return result\n",
        "#from save_text import *\n",
        "def save_array_to_txt(file, array):\n",
        "    saveFile=open(file,\"w\")\n",
        "    for item in array:\n",
        "        saveFile.write(str(item)+'\\n')\n",
        "    saveFile.close()\n",
        "#from get_structure import *\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def get_training_testing_sets(examples, testPercentage):\n",
        "    spamExamples=dict()\n",
        "    spam=0\n",
        "    hamExamples=dict()\n",
        "    ham=0\n",
        "    for example in examples:\n",
        "        if example[1]==1:\n",
        "            spamExamples[spam]=example\n",
        "            spam+=1\n",
        "        else:\n",
        "            hamExamples[ham]=example\n",
        "            ham+=1\n",
        "\n",
        "    trainingSet=[]\n",
        "    testingSet=[]\n",
        "            \n",
        "    testSpam=round((testPercentage*spam/100)/2)\n",
        "    randomSpam=np.arange(spam)\n",
        "    np.random.shuffle(randomSpam)\n",
        "\n",
        "    for i in range(len(randomSpam)):\n",
        "        #print(randomSpam[i])\n",
        "        if i <= testSpam:\n",
        "            testingSet.append(spamExamples[randomSpam[i]])\n",
        "        else:\n",
        "            trainingSet.append(spamExamples[randomSpam[i]])\n",
        "            \n",
        "    testHam=round((testPercentage*ham/100)/2)\n",
        "    randomHam = np.arange(ham)\n",
        "    np.random.shuffle(randomHam)\n",
        "\n",
        "    for i in range(len(randomHam)):\n",
        "        #print(randomHam[i])\n",
        "        if i <= testHam:\n",
        "            testingSet.append(hamExamples[randomHam[i]])\n",
        "        else:\n",
        "            trainingSet.append(hamExamples[randomHam[i]])\n",
        "            \n",
        "    return trainingSet, testingSet\n",
        "    \n",
        "def get_array_by_value(length, value):\n",
        "    array=[]\n",
        "    for i in range(length):\n",
        "        array.append([value])\n",
        "    return np.array(array)\n",
        "\n",
        "def get_array_with_types(text, types):\n",
        "    dictionary=dict()\n",
        "    for line in text.split('\\n'):\n",
        "        items = line.split()\n",
        "        numberItems=len(items)\n",
        "        if(numberItems>=2):\n",
        "            sentence=' '.join(c for c in items[:numberItems-1])\n",
        "            tag=items[numberItems-1]\n",
        "            dictionary[sentence]=(nltk.pos_tag(items[:numberItems-1]),types[tag])\n",
        "    return dictionary.values()\n",
        "\n",
        "def get_vocabulary(sentences):\n",
        "    words=[]\n",
        "    for sentence in sentences:\n",
        "        for word in sentence[0]:\n",
        "            words.append(word)\n",
        "    return sorted(set(words))\n",
        "    \n",
        "def get_frequencies_tags(sentences, vocabulary):\n",
        "    x=[]\n",
        "    y=[]\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        #x.append([1]+normalize_vector(get_frequency(vocabulary, sentence[0])))\n",
        "        #usar solo conteo\n",
        "        x.append([1]+get_frequency(vocabulary, sentence[0]))\n",
        "        y.append([sentence[1]])\n",
        "        \n",
        "    return np.array(x), np.array(y)\n",
        "    \n",
        "def get_frequency(vocabulary, vector):\n",
        "    frequency=[]\n",
        "    for w in vocabulary:\n",
        "        frequency.append(vector.count(w))\n",
        "    return frequency\n",
        "\n",
        "def normalize_vector(vector):\n",
        "    normalize=[]\n",
        "    sumVector=0\n",
        "    \n",
        "    for i in vector:\n",
        "        sumVector+=i\n",
        "    \n",
        "    if sumVector==0:\n",
        "        raise Exception(\"division by 0\")\n",
        "        \n",
        "    for i in vector:\n",
        "        normalize.append(i/sumVector)\n",
        "    \n",
        "    return normalize\n",
        "    \n",
        "def print_dictionary(dictionary):\n",
        "    for keys, values in dictionary.items():\n",
        "        print(keys)\n",
        "        print(values)\n",
        "\n",
        "types=Dic={\"ham\":0,\"spam\":1}\n",
        "text=get_text_from_txt('SMS_Spam_Corpus_big.txt')\n",
        "normalizeText=normalize_text(text)\n",
        "sentencesWithTags=get_array_with_types(normalizeText,types)\n",
        "trainingSet,testingSet=get_training_testing_sets(sentencesWithTags, 20)\n",
        "\n",
        "save_array_to_txt('.\\\\output\\\\trainingSet.txt', trainingSet)\n",
        "save_array_to_txt('.\\\\output\\\\testingSet.txt', testingSet)\n",
        "\n",
        "vocabulary=get_vocabulary(sentencesWithTags)\n",
        "trainingX,trainingY=get_frequencies_tags(trainingSet,vocabulary)\n",
        "m=len(trainingX)\n",
        "alpha=0.3\n",
        "iterations=250\n",
        "\n",
        "print('\\nAlpha   = ', alpha,'\\n')\n",
        "\n",
        "#Start training\n",
        "\n",
        "theta=get_array_by_value((1+len(vocabulary)), 0)\n",
        "for i in range(iterations):\n",
        "    z=np.dot(trainingX,theta)\n",
        "    hx=1/(1 + np.exp(-z))\n",
        "        \n",
        "    cost=(-1/m)*np.sum(np.multiply(trainingY,np.log(hx))+np.multiply(1-trainingY,np.log(1-hx)))\n",
        "    if (i+1)==1 or ((i+1)%50)==0:\n",
        "        print(\"{:25}\".format(str(i+1)) + 'cost =',cost)\n",
        "        \n",
        "    dTheta=(alpha/m)*np.dot(np.reshape((hx-trainingY),[1,m]),trainingX)\n",
        "    dTheta=dTheta.T\n",
        "    \n",
        "    theta=theta-dTheta\n",
        "\n",
        "#End training\n",
        "\n",
        "#Start testing\n",
        "\n",
        "testingX,testingY=get_frequencies_tags(testingSet,vocabulary)\n",
        "z=np.dot(testingX,theta)\n",
        "hx=1/(1 + np.exp(-z))\n",
        "y_pred=[]\n",
        "for i in hx:\n",
        "    if i >= 0.5:\n",
        "        y_pred.append([1])\n",
        "    else:\n",
        "        y_pred.append([0])\n",
        "\n",
        "TP=0\n",
        "FN=0\n",
        "FP=0\n",
        "TN=0\n",
        "spam=0\n",
        "ham=0\n",
        "support=len(testingY)\n",
        "\n",
        "for i in range(support):\n",
        "    if testingY[i] == 1:\n",
        "        spam+=1\n",
        "        if testingY[i] == y_pred[i]:\n",
        "            TP+=1\n",
        "        else:\n",
        "            FN+=1\n",
        "    else:\n",
        "        ham+=1\n",
        "        if testingY[i] == y_pred[i]:\n",
        "            TN+=1\n",
        "        else:\n",
        "            FP+=1\n",
        "\n",
        "#End testing\n",
        "\n",
        "confusionMatrix=pd.DataFrame({'': ['ham  = 0   Actual negative','spam = 1   Actual positive'], 'Positive prediction': [\"{:10}\".format('FP='+str(FP)),\"{:10}\".format('TP='+str(TP))], 'Negative prediction': [\"{:10}\".format('TN='+str(TN)),\"{:10}\".format('FN='+str(FN))], 'Tests': [ham,spam]},index=[' ', ' '])\n",
        "print('\\n')\n",
        "print(confusionMatrix)\n",
        "\n",
        "precisionHam=TN/(TN+FN)\n",
        "precisionSpam=TP/(TP+FP)\n",
        "\n",
        "recallHam=TN/(TN+FP)\n",
        "recallSpam=TP/(TP+FN)\n",
        "\n",
        "f1Ham=2*(precisionHam*recallHam)/(precisionHam+recallHam)\n",
        "f1Spam=2*(precisionSpam*recallSpam)/(precisionSpam+recallSpam)\n",
        "\n",
        "precisionMacroAvg=(precisionHam+precisionSpam)/2\n",
        "precisionWeightedAvg=(precisionHam*(ham/support))+(precisionSpam*(spam/support))\n",
        "\n",
        "recallMacroAvg=(recallHam+recallSpam)/2\n",
        "recallWeightedAvg=(recallHam*(ham/support))+(recallSpam*(spam/support))\n",
        "\n",
        "accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "\n",
        "f1MacroAvg=(f1Ham+f1Spam)/2\n",
        "f1WeightedAvg=(f1Ham*(ham/support))+(f1Spam*(spam/support))\n",
        "\n",
        "classificationReport=pd.DataFrame({'': [' ham 0', 'spam 1', ' ', 'accuracy','macro avg','weighted avg'],'precision': [round(precisionHam,2), round(precisionSpam,2), ' ', ' ',round(precisionMacroAvg,2),round(precisionWeightedAvg,2)],'recall': [round(recallHam,2), round(recallSpam,2), ' ', ' ',round(recallMacroAvg,2),round(recallWeightedAvg,2)],'f1-score': [round(f1Ham,2), round(f1Spam,2), ' ', round(accuracy,2),round(f1MacroAvg,2),round(f1WeightedAvg,2)],'support': [ham, spam, ' ', support,support,support]},index=[' ', ' ', ' ',' ', ' ', ' '])\n",
        "print('\\n')\n",
        "print(classificationReport)"
      ]
    }
  ]
}