{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpC6/gPPxzU7Z4WPfYV/8G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaronke/COMPUTATION/blob/main/Untitled34.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A1SXviMBf7a",
        "outputId": "8a10dbbc-76e9-4ab3-c640-7908b9c1cd70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        ">>> import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import nltk\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ra7g5GyCGYG",
        "outputId": "dcbe5339-efb6-4884-dd30-f5227b395eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXiOSHCDCXD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U06oK9J33Bwt",
        "outputId": "7940ac58-3656-429b-ccb8-328b49b43f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "                              Positive prediction Negative prediction  Tests\n",
            "   ham  = 0   Actual negative          FP=0                TN=1            1\n",
            "   spam = 1   Actual positive          TP=3                FN=1            4\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       ham 0       0.50      1.00      0.67         1\n",
            "      spam 1       1.00      0.75      0.86         4\n",
            "\n",
            "    accuracy                           0.80         5\n",
            "   macro avg       0.75      0.88      0.76         5\n",
            "weighted avg       0.90      0.80      0.82         5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#from get_text import *\n",
        "def get_text_from_txt(file, encode=\"latin-1\"):    \n",
        "    txt = open(file, \"r\", encoding=encode)\n",
        "    text = txt.read()\n",
        "    txt.close()\n",
        "    \n",
        "    return text\n",
        "#from normalize_text import *\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "        \n",
        "def normalize_text(text):\n",
        "    validCharacters=['á','é','í','ó','ú','ü','a','b','c','d','e','f','g','h','i','j','k','l','m','n','ñ','o','p','q','r','s','t','u','v','w','x','y','z', ' ', '\\n']\n",
        "    \n",
        "    # Pasar a minúsculas\n",
        "    normalizeText=text.lower()\n",
        "    \n",
        "    # Quitar números y caracteres no alfanuméricos\n",
        "    normalizeText=''.join(c for c in normalizeText if c in validCharacters)\n",
        "    \n",
        "    # Lematizar\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    result=''\n",
        "    lines=normalizeText.split('\\n')\n",
        "    numberLines=len(lines)\n",
        "    for i in range(numberLines):\n",
        "        for w in lines[i].split():\n",
        "            result+=lemmatizer.lemmatize(w)+' '\n",
        "        if (i < (numberLines-1)):\n",
        "            result+='\\n'\n",
        "    \n",
        "    return result\n",
        "\n",
        "#from save_text import *\n",
        "def save_array_to_txt(file, array):\n",
        "    saveFile=open(file,\"w\")\n",
        "    for item in array:\n",
        "        saveFile.write(str(item)+'\\n')\n",
        "    saveFile.close()\n",
        "\n",
        "#from get_structure import *\n",
        "def get_sentences_tags(text, types):\n",
        "    dictionary=dict()\n",
        "    for line in text.split('\\n'):\n",
        "        items = line.split()\n",
        "        numberItems=len(items)\n",
        "        if(numberItems>=2):\n",
        "            sentence=' '.join(c for c in items[:numberItems-1])\n",
        "            tag=items[numberItems-1]\n",
        "            dictionary[sentence]=types[tag]\n",
        "    return list(dictionary.keys()), list(dictionary.values())\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "types=Dic={\"ham\":0,\"spam\":1}\n",
        "text=get_text_from_txt('/content/olaa.txt')\n",
        "normalizeText=normalize_text(text)\n",
        "sentences,tags=get_sentences_tags(normalizeText,types)\n",
        "\n",
        "tf=CountVectorizer()\n",
        "data=tf.fit_transform(sentences)\n",
        "#print(tf.get_feature_names_out())\n",
        "#print(data.toarray())\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(data.toarray(),tags,test_size=0.2)\n",
        "\n",
        "#print(X_train,y_train)\n",
        "#print(X_test,y_test)\n",
        "\n",
        "clf=LogisticRegression(random_state=0,max_iter=250).fit(X_train,y_train)\n",
        "y_pred=clf.predict(X_test)\n",
        "#print(y_pred)\n",
        "\n",
        "TN,FP,FN,TP=confusion_matrix(y_test,y_pred).ravel()\n",
        "spam=y_test.count(1)\n",
        "ham=y_test.count(0)\n",
        "\n",
        "confusionMatrix=pd.DataFrame({'': ['ham  = 0   Actual negative','spam = 1   Actual positive'], 'Positive prediction': [\"{:10}\".format('FP='+str(FP)),\"{:10}\".format('TP='+str(TP))], 'Negative prediction': [\"{:10}\".format('TN='+str(TN)),\"{:10}\".format('FN='+str(FN))], 'Tests': [ham,spam]},index=[' ', ' '])\n",
        "print('\\n')\n",
        "print(confusionMatrix)\n",
        "\n",
        "target_names=['ham 0', 'spam 1']\n",
        "print('\\n')\n",
        "print(classification_report(y_test,y_pred,target_names=target_names))\n"
      ]
    }
  ]
}